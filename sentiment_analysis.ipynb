{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12141425,"sourceType":"datasetVersion","datasetId":7610209},{"sourceId":12230425,"sourceType":"datasetVersion","datasetId":7705993}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport geopandas as gpd\nfrom shapely.geometry import Point\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport folium\nimport seaborn as sns\nfrom folium.plugins import HeatMap\nfrom tqdm import tqdm\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-24T20:44:42.035098Z","iopub.execute_input":"2025-06-24T20:44:42.035404Z","iopub.status.idle":"2025-06-24T20:44:42.039676Z","shell.execute_reply.started":"2025-06-24T20:44:42.035384Z","shell.execute_reply":"2025-06-24T20:44:42.038815Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Note: you must run this notebook by:https://www.kaggle.com/code/stegosaurus3000/ml-sentiment","metadata":{}},{"cell_type":"markdown","source":"# Sentimental Analysis","metadata":{}},{"cell_type":"code","source":"listings = pd.read_csv(\"/kaggle/input/cleaned-df-gesa-csv/airbnb_cleaned_for_ML.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T20:44:42.040898Z","iopub.execute_input":"2025-06-24T20:44:42.041156Z","iopub.status.idle":"2025-06-24T20:44:42.150242Z","shell.execute_reply.started":"2025-06-24T20:44:42.041140Z","shell.execute_reply":"2025-06-24T20:44:42.149635Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"reviews = pd.read_csv('/kaggle/input/florence-airbnb-data/reviews.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T20:45:01.180203Z","iopub.execute_input":"2025-06-24T20:45:01.180453Z","iopub.status.idle":"2025-06-24T20:45:06.889091Z","shell.execute_reply.started":"2025-06-24T20:45:01.180434Z","shell.execute_reply":"2025-06-24T20:45:06.888484Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We split long reviews into smaller parts because the model can only handle 512 tokens at a time. Then, we run sentiment analysis on each part using a pretrained multilingual transformer model and average the results to get one sentiment score per review","metadata":{}},{"cell_type":"code","source":"# Keep only necessary \nreviews_filtered = reviews[['listing_id', 'comments']].dropna()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T20:45:06.889822Z","iopub.execute_input":"2025-06-24T20:45:06.890022Z","iopub.status.idle":"2025-06-24T20:45:07.102012Z","shell.execute_reply.started":"2025-06-24T20:45:06.890007Z","shell.execute_reply":"2025-06-24T20:45:07.101338Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\n\ndef clean_text(text):\n    # Lowercase\n    text = text.lower()\n    # Remove emojis and special characters (except basic punctuation)\n    text = re.sub(r'[^\\w\\s,.!?]', '', text)\n    # Remove extra whitespace\n    text = re.sub(r'\\s+', ' ', text).strip()\n    return text\n\nreviews_filtered['clean_comments'] = reviews_filtered['comments'].apply(clean_text)\n\nreviews_filtered[['comments', 'clean_comments']]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T20:45:07.102846Z","iopub.execute_input":"2025-06-24T20:45:07.103440Z","iopub.status.idle":"2025-06-24T20:45:30.775080Z","shell.execute_reply.started":"2025-06-24T20:45:07.103414Z","shell.execute_reply":"2025-06-24T20:45:30.774272Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport gc\n\n# Setup simple and fast\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using: {device}\")\n\nmodel_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name).to(device)\nmodel.eval()\n\ndef get_sentiment_scores_fast(texts, batch_size=256, chunk_size=50000):\n    \"\"\"\n    Simple sentiment analysis in chunks for 1M reviews\n    Returns float scores from 1.0 to 5.0\n    \"\"\"\n    print(f\"Processing {len(texts):,} reviews in chunks of {chunk_size:,}\")\n    \n    all_scores = []\n    \n    # Process in chunks to avoid memory issues\n    for chunk_start in range(0, len(texts), chunk_size):\n        chunk_end = min(chunk_start + chunk_size, len(texts))\n        chunk_texts = texts[chunk_start:chunk_end]\n        \n        print(f\"Processing chunk {chunk_start//chunk_size + 1}/{(len(texts)-1)//chunk_size + 1}\")\n        \n        chunk_scores = []\n        \n        # Process chunk in batches\n        for i in tqdm(range(0, len(chunk_texts), batch_size), desc=\"Batches\"):\n            batch_texts = chunk_texts[i:i+batch_size]\n            \n            # Clean and prepare texts\n            clean_batch = []\n            for text in batch_texts:\n                if text is None or not isinstance(text, str):\n                    clean_batch.append(\"\")\n                else:\n                    clean_batch.append(str(text)[:512])  # Truncate long texts\n            \n            # Tokenize\n            inputs = tokenizer(\n                clean_batch,\n                padding=True,\n                truncation=True,\n                max_length=512,\n                return_tensors=\"pt\"\n            ).to(device)\n            \n            # Predict\n            with torch.no_grad():\n                outputs = model(**inputs)\n                predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n                predicted_classes = torch.argmax(predictions, dim=-1)\n                \n                # Convert to 1-5 scale (model outputs 0-4)\n                scores = (predicted_classes + 1).float().cpu().numpy()\n                chunk_scores.extend(scores)\n            \n            # Clean memory every 20 batches\n            if i % (batch_size * 20) == 0:\n                torch.cuda.empty_cache()\n        \n        all_scores.extend(chunk_scores)\n        \n        # Clean memory after each chunk\n        torch.cuda.empty_cache()\n        gc.collect()\n        \n        print(f\"Chunk completed. Total processed: {len(all_scores):,}\")\n    \n    return [float(score) for score in all_scores]\n\n# Apply to reviews_filtered\nprint(\"Starting sentiment analysis on reviews_filtered...\")\n\n# Get the scores\nsentiment_scores = get_sentiment_scores_fast(\n    reviews_filtered['clean_comments'].tolist(),\n    batch_size=256,\n    chunk_size=50000  # Process 50K at a time\n)\n\n# Add to dataframe as float\nreviews_filtered['sentiment_score'] = sentiment_scores\n\nprint(\"Sentiment scores added as float values (1.0 to 5.0)\")\nprint(f\"Score distribution:\")\nprint(f\"Mean: {np.mean(sentiment_scores):.2f}\")\nprint(f\"Min: {min(sentiment_scores):.1f}, Max: {max(sentiment_scores):.1f}\")\n\n# Show sample\nprint(\"\\nSample results:\")\nprint(reviews_filtered[['clean_comments', 'sentiment_score']].head())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"reviews_filtered.to_csv('/kaggle/working/scored_reviews.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T14:41:26.777743Z","iopub.execute_input":"2025-06-25T14:41:26.777960Z","iopub.status.idle":"2025-06-25T14:41:26.852087Z","shell.execute_reply.started":"2025-06-25T14:41:26.777938Z","shell.execute_reply":"2025-06-25T14:41:26.851187Z"}},"outputs":[],"execution_count":null}]}